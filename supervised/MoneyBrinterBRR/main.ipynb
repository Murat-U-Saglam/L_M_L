{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "from lightgbm import LGBMRegressor\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from numerapi import NumerAPI \n",
    "from utils import (\n",
    "    save_model,\n",
    "    load_model,\n",
    "    neutralize,\n",
    "    validation_metrics,\n",
    "    ERA_COL,\n",
    "    DATA_TYPE_COL,\n",
    "    TARGET_COL,\n",
    "    EXAMPLE_PREDS_COL,\n",
    ")\n",
    "\n",
    "# Authenticate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "\r"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "env_vars = dotenv_values('.env')\n",
    "napi = NumerAPI(env_vars['PUBLIC_ID'], env_vars['PRIVATE_KEY'])\n",
    "current_round = napi.get_current_round()\n",
    "#Dataset path\n",
    "version = \"v4.1\"\n",
    "feature_set_name = \"medium\"\n",
    "train_path = f\"{version}/train.parquet\"\n",
    "validation_path = f\"{version}/validation.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 10:01:22,822 INFO numerapi.utils: target file already exists\n",
      "2023-07-18 10:01:22,822 INFO numerapi.utils: download complete\n",
      "2023-07-18 10:01:24,058 INFO numerapi.utils: target file already exists\n",
      "2023-07-18 10:01:24,058 INFO numerapi.utils: download complete\n",
      "2023-07-18 10:01:25,384 INFO numerapi.utils: target file already exists\n",
      "2023-07-18 10:01:25,384 INFO numerapi.utils: download complete\n",
      "2023-07-18 10:01:26,518 INFO numerapi.utils: target file already exists\n",
      "2023-07-18 10:01:26,519 INFO numerapi.utils: download complete\n",
      "2023-07-18 10:01:27,715 INFO numerapi.utils: target file already exists\n",
      "2023-07-18 10:01:27,716 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'v4.1/features.json'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "napi.download_dataset(f\"{version}/train.parquet\")\n",
    "napi.download_dataset(f\"{version}/validation.parquet\")\n",
    "napi.download_dataset(f\"{version}/live.parquet\", f\"{version}/live_{current_round}.parquet\")\n",
    "\n",
    "napi.download_dataset(f\"{version}/validation_example_preds.parquet\")\n",
    "napi.download_dataset(f\"{version}/features.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "with open(f\"{version}/features.json\", \"r\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "\n",
    "features = feature_metadata[\"feature_sets\"][\n",
    "    feature_set_name\n",
    "] \n",
    "target_cols = feature_metadata[\"targets\"]\n",
    "read_columns = features + target_cols + [ERA_COL, DATA_TYPE_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_parquet(\n",
    "    f\"{version}/train.parquet\", columns=read_columns, engine='fastparquet'\n",
    ")\n",
    "validation_data = pd.read_parquet(\n",
    "    f\"{version}/validation.parquet\", columns=read_columns, engine='fastparquet'\n",
    ")\n",
    "live_data = pd.read_parquet(f\"{version}/live_{current_round}.parquet\", columns=read_columns, engine='fastparquet'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Final model comment this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# reduce the number of eras to every 4th era to speed things up... uncomment these lines to speed things up.\n",
    "every_4th_era = training_data[ERA_COL].unique()[::4]\n",
    "training_data = training_data[training_data[ERA_COL].isin(every_4th_era)]\n",
    "every_4th_era = validation_data[ERA_COL].unique()[::4]\n",
    "validation_data = validation_data[validation_data[ERA_COL].isin(every_4th_era)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# get all the data to possibly use for training\n",
    "all_data = pd.concat([training_data, validation_data])\n",
    "all_data[features] = all_data[features].fillna(all_data[features].median(skipna=True))\n",
    "live_data[features] = live_data[features].fillna(\n",
    "    all_data[features].median(skipna=True)\n",
    ") # use the training data median to fill in live data missing values\n",
    "training_index = training_data.index\n",
    "validation_index = validation_data.index\n",
    "all_index = all_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data\n",
      "\n",
      "\n",
      "shape: (1243809, 680)\n",
      " Index(['feature_abating_unadaptable_weakfish',\n",
      "       'feature_ablest_mauritanian_elding',\n",
      "       'feature_acclimatisable_unfeigned_maghreb',\n",
      "       'feature_accommodable_crinite_cleft',\n",
      "       'feature_accretive_sorrier_skedaddle',\n",
      "       'feature_acetose_periotic_coronation',\n",
      "       'feature_additive_untrustworthy_hierologist',\n",
      "       'feature_adsorbed_blizzardy_burlesque',\n",
      "       'feature_affettuoso_taxidermic_greg', 'feature_afoul_valvate_faery',\n",
      "       ...\n",
      "       'target_cyrus_v4_20', 'target_cyrus_v4_60', 'target_caroline_v4_20',\n",
      "       'target_caroline_v4_60', 'target_sam_v4_20', 'target_sam_v4_60',\n",
      "       'target_xerxes_v4_20', 'target_xerxes_v4_60', 'era', 'data_type'],\n",
      "      dtype='object', length=680)\n",
      " <bound method NDFrame.describe of                   feature_abating_unadaptable_weakfish  \\\n",
      "id                                                       \n",
      "n003bba8a98662e4                                  0.00   \n",
      "n003bee128c2fcfc                                  1.00   \n",
      "n0048ac83aff7194                                  1.00   \n",
      "n00691bec80d3e02                                  0.25   \n",
      "n00b8720a2fdc4f2                                  0.00   \n",
      "...                                                ...   \n",
      "nffd5af15959f152                                  0.75   \n",
      "nffd9899640fa670                                  0.00   \n",
      "nffdc9ed5105d9c3                                  0.75   \n",
      "nfff1b65066d6a16                                  0.75   \n",
      "nfff71d3516eaf1d                                  1.00   \n",
      "\n",
      "                  feature_ablest_mauritanian_elding  \\\n",
      "id                                                    \n",
      "n003bba8a98662e4                               1.00   \n",
      "n003bee128c2fcfc                               0.50   \n",
      "n0048ac83aff7194                               1.00   \n",
      "n00691bec80d3e02                               1.00   \n",
      "n00b8720a2fdc4f2                               0.50   \n",
      "...                                             ...   \n",
      "nffd5af15959f152                               0.25   \n",
      "nffd9899640fa670                               0.25   \n",
      "nffdc9ed5105d9c3                               0.75   \n",
      "nfff1b65066d6a16                               0.00   \n",
      "nfff71d3516eaf1d                               0.00   \n",
      "\n",
      "                  feature_acclimatisable_unfeigned_maghreb  \\\n",
      "id                                                           \n",
      "n003bba8a98662e4                                      0.00   \n",
      "n003bee128c2fcfc                                      0.50   \n",
      "n0048ac83aff7194                                      0.50   \n",
      "n00691bec80d3e02                                      0.25   \n",
      "n00b8720a2fdc4f2                                      0.00   \n",
      "...                                                    ...   \n",
      "nffd5af15959f152                                      0.75   \n",
      "nffd9899640fa670                                      0.00   \n",
      "nffdc9ed5105d9c3                                      0.00   \n",
      "nfff1b65066d6a16                                      0.00   \n",
      "nfff71d3516eaf1d                                      0.25   \n",
      "\n",
      "                  feature_accommodable_crinite_cleft  \\\n",
      "id                                                     \n",
      "n003bba8a98662e4                                1.00   \n",
      "n003bee128c2fcfc                                0.50   \n",
      "n0048ac83aff7194                                0.00   \n",
      "n00691bec80d3e02                                0.25   \n",
      "n00b8720a2fdc4f2                                0.00   \n",
      "...                                              ...   \n",
      "nffd5af15959f152                                1.00   \n",
      "nffd9899640fa670                                0.75   \n",
      "nffdc9ed5105d9c3                                0.75   \n",
      "nfff1b65066d6a16                                0.25   \n",
      "nfff71d3516eaf1d                                0.25   \n",
      "\n",
      "                  feature_accretive_sorrier_skedaddle  \\\n",
      "id                                                      \n",
      "n003bba8a98662e4                                 0.50   \n",
      "n003bee128c2fcfc                                 0.50   \n",
      "n0048ac83aff7194                                 0.50   \n",
      "n00691bec80d3e02                                 0.50   \n",
      "n00b8720a2fdc4f2                                 0.50   \n",
      "...                                               ...   \n",
      "nffd5af15959f152                                 0.25   \n",
      "nffd9899640fa670                                 0.25   \n",
      "nffdc9ed5105d9c3                                 0.00   \n",
      "nfff1b65066d6a16                                 0.25   \n",
      "nfff71d3516eaf1d                                 0.25   \n",
      "\n",
      "                  feature_acetose_periotic_coronation  \\\n",
      "id                                                      \n",
      "n003bba8a98662e4                                 0.00   \n",
      "n003bee128c2fcfc                                 0.75   \n",
      "n0048ac83aff7194                                 0.00   \n",
      "n00691bec80d3e02                                 0.00   \n",
      "n00b8720a2fdc4f2                                 0.00   \n",
      "...                                               ...   \n",
      "nffd5af15959f152                                 1.00   \n",
      "nffd9899640fa670                                 0.25   \n",
      "nffdc9ed5105d9c3                                 0.25   \n",
      "nfff1b65066d6a16                                 0.50   \n",
      "nfff71d3516eaf1d                                 0.50   \n",
      "\n",
      "                  feature_additive_untrustworthy_hierologist  \\\n",
      "id                                                             \n",
      "n003bba8a98662e4                                        0.25   \n",
      "n003bee128c2fcfc                                        0.25   \n",
      "n0048ac83aff7194                                        1.00   \n",
      "n00691bec80d3e02                                        0.50   \n",
      "n00b8720a2fdc4f2                                        0.75   \n",
      "...                                                      ...   \n",
      "nffd5af15959f152                                        0.25   \n",
      "nffd9899640fa670                                        0.00   \n",
      "nffdc9ed5105d9c3                                        1.00   \n",
      "nfff1b65066d6a16                                        0.50   \n",
      "nfff71d3516eaf1d                                        0.25   \n",
      "\n",
      "                  feature_adsorbed_blizzardy_burlesque  \\\n",
      "id                                                       \n",
      "n003bba8a98662e4                                  1.00   \n",
      "n003bee128c2fcfc                                  0.75   \n",
      "n0048ac83aff7194                                  0.25   \n",
      "n00691bec80d3e02                                  0.25   \n",
      "n00b8720a2fdc4f2                                  0.25   \n",
      "...                                                ...   \n",
      "nffd5af15959f152                                  1.00   \n",
      "nffd9899640fa670                                  0.25   \n",
      "nffdc9ed5105d9c3                                  0.25   \n",
      "nfff1b65066d6a16                                  0.25   \n",
      "nfff71d3516eaf1d                                  0.00   \n",
      "\n",
      "                  feature_affettuoso_taxidermic_greg  \\\n",
      "id                                                     \n",
      "n003bba8a98662e4                                0.00   \n",
      "n003bee128c2fcfc                                0.50   \n",
      "n0048ac83aff7194                                0.75   \n",
      "n00691bec80d3e02                                0.50   \n",
      "n00b8720a2fdc4f2                                0.00   \n",
      "...                                              ...   \n",
      "nffd5af15959f152                                0.75   \n",
      "nffd9899640fa670                                0.00   \n",
      "nffdc9ed5105d9c3                                0.00   \n",
      "nfff1b65066d6a16                                0.25   \n",
      "nfff71d3516eaf1d                                0.25   \n",
      "\n",
      "                  feature_afoul_valvate_faery  ...  target_cyrus_v4_20  \\\n",
      "id                                             ...                       \n",
      "n003bba8a98662e4                         0.75  ...                0.25   \n",
      "n003bee128c2fcfc                         0.50  ...                0.75   \n",
      "n0048ac83aff7194                         0.50  ...                0.25   \n",
      "n00691bec80d3e02                         1.00  ...                0.75   \n",
      "n00b8720a2fdc4f2                         0.50  ...                0.50   \n",
      "...                                       ...  ...                 ...   \n",
      "nffd5af15959f152                         0.75  ...                 NaN   \n",
      "nffd9899640fa670                         0.25  ...                 NaN   \n",
      "nffdc9ed5105d9c3                         0.50  ...                 NaN   \n",
      "nfff1b65066d6a16                         0.25  ...                 NaN   \n",
      "nfff71d3516eaf1d                         0.75  ...                 NaN   \n",
      "\n",
      "                  target_cyrus_v4_60  target_caroline_v4_20  \\\n",
      "id                                                            \n",
      "n003bba8a98662e4                0.25                   0.25   \n",
      "n003bee128c2fcfc                0.75                   0.75   \n",
      "n0048ac83aff7194                0.25                   0.50   \n",
      "n00691bec80d3e02                0.50                   0.75   \n",
      "n00b8720a2fdc4f2                0.50                   0.50   \n",
      "...                              ...                    ...   \n",
      "nffd5af15959f152                 NaN                    NaN   \n",
      "nffd9899640fa670                 NaN                    NaN   \n",
      "nffdc9ed5105d9c3                 NaN                    NaN   \n",
      "nfff1b65066d6a16                 NaN                    NaN   \n",
      "nfff71d3516eaf1d                 NaN                    NaN   \n",
      "\n",
      "                  target_caroline_v4_60  target_sam_v4_20  target_sam_v4_60  \\\n",
      "id                                                                            \n",
      "n003bba8a98662e4                   0.25              0.25              0.25   \n",
      "n003bee128c2fcfc                   0.75              0.75              0.75   \n",
      "n0048ac83aff7194                   0.25              0.50              0.25   \n",
      "n00691bec80d3e02                   0.50              0.75              0.50   \n",
      "n00b8720a2fdc4f2                   0.50              0.50              0.50   \n",
      "...                                 ...               ...               ...   \n",
      "nffd5af15959f152                    NaN               NaN               NaN   \n",
      "nffd9899640fa670                    NaN               NaN               NaN   \n",
      "nffdc9ed5105d9c3                    NaN               NaN               NaN   \n",
      "nfff1b65066d6a16                    NaN               NaN               NaN   \n",
      "nfff71d3516eaf1d                    NaN               NaN               NaN   \n",
      "\n",
      "                  target_xerxes_v4_20  target_xerxes_v4_60   era  data_type  \n",
      "id                                                                           \n",
      "n003bba8a98662e4                 0.25                 0.25  0001      train  \n",
      "n003bee128c2fcfc                 0.75                 0.75  0001      train  \n",
      "n0048ac83aff7194                 0.25                 0.25  0001      train  \n",
      "n00691bec80d3e02                 0.75                 0.50  0001      train  \n",
      "n00b8720a2fdc4f2                 0.50                 0.50  0001      train  \n",
      "...                               ...                  ...   ...        ...  \n",
      "nffd5af15959f152                  NaN                  NaN  1071       test  \n",
      "nffd9899640fa670                  NaN                  NaN  1071       test  \n",
      "nffdc9ed5105d9c3                  NaN                  NaN  1071       test  \n",
      "nfff1b65066d6a16                  NaN                  NaN  1071       test  \n",
      "nfff71d3516eaf1d                  NaN                  NaN  1071       test  \n",
      "\n",
      "[1243809 rows x 680 columns]>\n",
      "\r"
     ]
    }
   ],
   "source": [
    "print(f\"all_data\\n\\n\")\n",
    "print(f\"shape: {all_data.shape}\\n {all_data.columns}\\n {all_data.describe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# delete training and validation data to save space\n",
    "del training_data\n",
    "del validation_data\n",
    "gc.collect()  # clear up memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "param_id = \"lg_lgbm\"\n",
    "params = {\n",
    "    \"n_estimators\": 20000,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_depth\": 6,\n",
    "    \"num_leaves\": 2**6,\n",
    "    \"colsample_bytree\": 0.1,\n",
    "    \"objective\": \"regression\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['target', 'target_nomi_v4_20', 'target_nomi_v4_60', 'target_tyler_v4_20', 'target_tyler_v4_60', 'target_victor_v4_20', 'target_victor_v4_60', 'target_ralph_v4_20', 'target_ralph_v4_60', 'target_waldo_v4_20', 'target_waldo_v4_60', 'target_jerome_v4_20', 'target_jerome_v4_60', 'target_janet_v4_20', 'target_janet_v4_60', 'target_ben_v4_20', 'target_ben_v4_60', 'target_alan_v4_20', 'target_alan_v4_60', 'target_paul_v4_20', 'target_paul_v4_60', 'target_george_v4_20', 'target_george_v4_60', 'target_william_v4_20', 'target_william_v4_60', 'target_arthur_v4_20', 'target_arthur_v4_60', 'target_thomas_v4_20', 'target_thomas_v4_60', 'target_cyrus_v4_20', 'target_cyrus_v4_60', 'target_caroline_v4_20', 'target_caroline_v4_60', 'target_sam_v4_20', 'target_sam_v4_60', 'target_xerxes_v4_20', 'target_xerxes_v4_60']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      "  3%|▎         | 1/37 [00:02<01:44,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_nomi_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_nomi_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      "  5%|▌         | 2/37 [00:05<01:40,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_nomi_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_nomi_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      "  8%|▊         | 3/37 [00:08<01:41,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_tyler_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_tyler_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 11%|█         | 4/37 [00:10<01:26,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_tyler_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_tyler_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 14%|█▎        | 5/37 [00:12<01:17,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_victor_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_victor_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 16%|█▌        | 6/37 [00:15<01:11,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_victor_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_victor_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 19%|█▉        | 7/37 [00:18<01:23,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_ralph_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_ralph_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 22%|██▏       | 8/37 [00:22<01:25,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_ralph_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_ralph_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 24%|██▍       | 9/37 [00:25<01:29,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_waldo_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_waldo_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 27%|██▋       | 10/37 [00:27<01:16,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_waldo_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_waldo_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 30%|██▉       | 11/37 [00:29<01:06,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_jerome_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_jerome_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 32%|███▏      | 12/37 [00:32<01:01,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_jerome_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_jerome_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 35%|███▌      | 13/37 [00:34<00:56,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_janet_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_janet_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 38%|███▊      | 14/37 [00:37<01:01,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_janet_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_janet_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 41%|████      | 15/37 [00:40<01:02,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_ben_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_ben_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 43%|████▎     | 16/37 [00:43<01:00,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_ben_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_ben_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 46%|████▌     | 17/37 [00:45<00:53,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_alan_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_alan_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 49%|████▊     | 18/37 [00:48<00:48,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_alan_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_alan_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 51%|█████▏    | 19/37 [00:50<00:44,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_paul_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_paul_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 54%|█████▍    | 20/37 [00:52<00:41,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_paul_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_paul_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 57%|█████▋    | 21/37 [00:55<00:37,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_george_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_george_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 59%|█████▉    | 22/37 [00:57<00:34,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_george_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_george_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 62%|██████▏   | 23/37 [00:59<00:32,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_william_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_william_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 65%|██████▍   | 24/37 [01:01<00:29,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_william_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_william_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 68%|██████▊   | 25/37 [01:03<00:27,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_arthur_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_arthur_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 70%|███████   | 26/37 [01:07<00:27,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_arthur_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_arthur_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 73%|███████▎  | 27/37 [01:09<00:24,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_thomas_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_thomas_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 76%|███████▌  | 28/37 [01:11<00:21,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_thomas_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_thomas_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 78%|███████▊  | 29/37 [01:14<00:19,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_cyrus_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_cyrus_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 81%|████████  | 30/37 [01:18<00:20,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_cyrus_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_cyrus_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 84%|████████▍ | 31/37 [01:20<00:16,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_caroline_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_caroline_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 86%|████████▋ | 32/37 [01:22<00:12,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_caroline_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_caroline_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 89%|████████▉ | 33/37 [01:24<00:09,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_sam_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_sam_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 92%|█████████▏| 34/37 [01:26<00:06,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_sam_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_sam_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 95%|█████████▍| 35/37 [01:28<00:04,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_xerxes_v4_20'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_xerxes_v4_20'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      " 97%|█████████▋| 36/37 [01:30<00:02,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'train_data_lg_lgbm_v4.1_medium_target_xerxes_v4_60'\n",
      "Checking for existing model 'all_data_lg_lgbm_v4.1_medium_target_xerxes_v4_60'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/1429479199.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  live_data[prediction_col] = all_data_model.predict(\n",
      "100%|██████████| 37/37 [01:31<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "feature = []\n",
    "target_cols = []\n",
    "prediction_cols = []\n",
    "for col in all_data:\n",
    "    if col.startswith(\"feature\"):\n",
    "        feature.append(col)\n",
    "    elif col.startswith(\"target\"):\n",
    "        target_cols.append(col)\n",
    "\n",
    "print(target_cols)\n",
    "\n",
    "for target in tqdm(target_cols):\n",
    "    prediction_col = f\"{param_id}_{version}_{feature_set_name}_{target}\"\n",
    "    train_data_model_name = f\"train_data_{prediction_col}\"\n",
    "    print(f\"Checking for existing model '{train_data_model_name}'\")\n",
    "    train_model = load_model(train_data_model_name)\n",
    "    if not train_model:\n",
    "        print(f\"Training model '{train_data_model_name}'\")\n",
    "        train_model = LGBMRegressor(**params)\n",
    "        train_model.fit(\n",
    "            all_data.loc[training_index, feature],\n",
    "            all_data.loc[training_index, target],\n",
    "            eval_set=[(all_data.loc[validation_index, feature], all_data.loc[validation_index, target])],\n",
    "            early_stopping_rounds=200,\n",
    "            verbose=100,\n",
    "        )\n",
    "        print(f\"saving new model: {train_data_model_name}\")\n",
    "        save_model(train_model, train_data_model_name)\n",
    "\n",
    "    all_data_model_name = f\"all_data_{prediction_col}\"\n",
    "    print(f\"Checking for existing model '{all_data_model_name}'\")\n",
    "    all_data_model = load_model(all_data_model_name)\n",
    "    if not all_data_model:\n",
    "        print(f\"model not found, creating new one\")\n",
    "        all_data_model = LGBMRegressor(**params)\n",
    "        all_data_model.fit(\n",
    "            all_data.loc[all_index, features],\n",
    "            all_data.loc[all_index, target],\n",
    "        )\n",
    "        print(f\"saving new model: {all_data_model_name}\")\n",
    "        save_model(all_data_model, all_data_model_name)\n",
    "    \n",
    "        # predict on live data\n",
    "    live_data[prediction_col] = all_data_model.predict(\n",
    "        live_data[features].fillna(np.nan)\n",
    "    )  # filling live data with nans makes us ignore those features if necessary\n",
    "    gc.collect()\n",
    "\n",
    "    prediction_cols.append(prediction_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feature_abating_unadaptable_weakfish',\n",
      "       'feature_ablest_mauritanian_elding',\n",
      "       'feature_acclimatisable_unfeigned_maghreb',\n",
      "       'feature_accommodable_crinite_cleft',\n",
      "       'feature_accretive_sorrier_skedaddle',\n",
      "       'feature_acetose_periotic_coronation',\n",
      "       'feature_additive_untrustworthy_hierologist',\n",
      "       'feature_adsorbed_blizzardy_burlesque',\n",
      "       'feature_affettuoso_taxidermic_greg', 'feature_afoul_valvate_faery',\n",
      "       ...\n",
      "       'target_cyrus_v4_20', 'target_cyrus_v4_60', 'target_caroline_v4_20',\n",
      "       'target_caroline_v4_60', 'target_sam_v4_20', 'target_sam_v4_60',\n",
      "       'target_xerxes_v4_20', 'target_xerxes_v4_60', 'era', 'data_type'],\n",
      "      dtype='object', length=680)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['lg_lgbm_v4.1_medium_target', 'lg_lgbm_v4.1_medium_target_nomi_v4_20',\\n       'lg_lgbm_v4.1_medium_target_nomi_v4_60',\\n       'lg_lgbm_v4.1_medium_target_tyler_v4_20',\\n       'lg_lgbm_v4.1_medium_target_tyler_v4_60',\\n       'lg_lgbm_v4.1_medium_target_victor_v4_20',\\n       'lg_lgbm_v4.1_medium_target_victor_v4_60',\\n       'lg_lgbm_v4.1_medium_target_ralph_v4_20',\\n       'lg_lgbm_v4.1_medium_target_ralph_v4_60',\\n       'lg_lgbm_v4.1_medium_target_waldo_v4_20',\\n       'lg_lgbm_v4.1_medium_target_waldo_v4_60',\\n       'lg_lgbm_v4.1_medium_target_jerome_v4_20',\\n       'lg_lgbm_v4.1_medium_target_jerome_v4_60',\\n       'lg_lgbm_v4.1_medium_target_janet_v4_20',\\n       'lg_lgbm_v4.1_medium_target_janet_v4_60',\\n       'lg_lgbm_v4.1_medium_target_ben_v4_20',\\n       'lg_lgbm_v4.1_medium_target_ben_v4_60',\\n       'lg_lgbm_v4.1_medium_target_alan_v4_20',\\n       'lg_lgbm_v4.1_medium_target_alan_v4_60',\\n       'lg_lgbm_v4.1_medium_target_paul_v4_20',\\n       'lg_lgbm_v4.1_medium_target_paul_v4_60',\\n       'lg_lgbm_v4.1_medium_target_george_v4_20',\\n       'lg_lgbm_v4.1_medium_target_george_v4_60',\\n       'lg_lgbm_v4.1_medium_target_william_v4_20',\\n       'lg_lgbm_v4.1_medium_target_william_v4_60',\\n       'lg_lgbm_v4.1_medium_target_arthur_v4_20',\\n       'lg_lgbm_v4.1_medium_target_arthur_v4_60',\\n       'lg_lgbm_v4.1_medium_target_thomas_v4_20',\\n       'lg_lgbm_v4.1_medium_target_thomas_v4_60',\\n       'lg_lgbm_v4.1_medium_target_cyrus_v4_20',\\n       'lg_lgbm_v4.1_medium_target_cyrus_v4_60',\\n       'lg_lgbm_v4.1_medium_target_caroline_v4_20',\\n       'lg_lgbm_v4.1_medium_target_caroline_v4_60',\\n       'lg_lgbm_v4.1_medium_target_sam_v4_20',\\n       'lg_lgbm_v4.1_medium_target_sam_v4_60',\\n       'lg_lgbm_v4.1_medium_target_xerxes_v4_20',\\n       'lg_lgbm_v4.1_medium_target_xerxes_v4_60'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# make an ensemble -- Aggregating all models\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(all_data\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m----> 3\u001b[0m all_data\u001b[39m.\u001b[39mloc[:, \u001b[39m\"\u001b[39m\u001b[39mequal_weight\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m all_data[prediction_cols]\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m live_data[\u001b[39m\"\u001b[39m\u001b[39mequal_weight\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m live_data[prediction_cols]\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m prediction_cols\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mequal_weight\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/ML-Stuff/venv/lib/python3.10/site-packages/pandas/core/frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3766\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3767\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3769\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/ML-Stuff/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5877\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5879\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5880\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5881\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ML-Stuff/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5936\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5937\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 5938\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5940\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   5941\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['lg_lgbm_v4.1_medium_target', 'lg_lgbm_v4.1_medium_target_nomi_v4_20',\\n       'lg_lgbm_v4.1_medium_target_nomi_v4_60',\\n       'lg_lgbm_v4.1_medium_target_tyler_v4_20',\\n       'lg_lgbm_v4.1_medium_target_tyler_v4_60',\\n       'lg_lgbm_v4.1_medium_target_victor_v4_20',\\n       'lg_lgbm_v4.1_medium_target_victor_v4_60',\\n       'lg_lgbm_v4.1_medium_target_ralph_v4_20',\\n       'lg_lgbm_v4.1_medium_target_ralph_v4_60',\\n       'lg_lgbm_v4.1_medium_target_waldo_v4_20',\\n       'lg_lgbm_v4.1_medium_target_waldo_v4_60',\\n       'lg_lgbm_v4.1_medium_target_jerome_v4_20',\\n       'lg_lgbm_v4.1_medium_target_jerome_v4_60',\\n       'lg_lgbm_v4.1_medium_target_janet_v4_20',\\n       'lg_lgbm_v4.1_medium_target_janet_v4_60',\\n       'lg_lgbm_v4.1_medium_target_ben_v4_20',\\n       'lg_lgbm_v4.1_medium_target_ben_v4_60',\\n       'lg_lgbm_v4.1_medium_target_alan_v4_20',\\n       'lg_lgbm_v4.1_medium_target_alan_v4_60',\\n       'lg_lgbm_v4.1_medium_target_paul_v4_20',\\n       'lg_lgbm_v4.1_medium_target_paul_v4_60',\\n       'lg_lgbm_v4.1_medium_target_george_v4_20',\\n       'lg_lgbm_v4.1_medium_target_george_v4_60',\\n       'lg_lgbm_v4.1_medium_target_william_v4_20',\\n       'lg_lgbm_v4.1_medium_target_william_v4_60',\\n       'lg_lgbm_v4.1_medium_target_arthur_v4_20',\\n       'lg_lgbm_v4.1_medium_target_arthur_v4_60',\\n       'lg_lgbm_v4.1_medium_target_thomas_v4_20',\\n       'lg_lgbm_v4.1_medium_target_thomas_v4_60',\\n       'lg_lgbm_v4.1_medium_target_cyrus_v4_20',\\n       'lg_lgbm_v4.1_medium_target_cyrus_v4_60',\\n       'lg_lgbm_v4.1_medium_target_caroline_v4_20',\\n       'lg_lgbm_v4.1_medium_target_caroline_v4_60',\\n       'lg_lgbm_v4.1_medium_target_sam_v4_20',\\n       'lg_lgbm_v4.1_medium_target_sam_v4_60',\\n       'lg_lgbm_v4.1_medium_target_xerxes_v4_20',\\n       'lg_lgbm_v4.1_medium_target_xerxes_v4_60'],\\n      dtype='object')] are in the [columns]\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# make an ensemble -- Aggregating all models\n",
    "print(all_data.columns)\n",
    "all_data.loc[:, \"equal_weight\"] = all_data[prediction_cols].mean(axis=1)\n",
    "live_data[\"equal_weight\"] = live_data[prediction_cols].mean(axis=1)\n",
    "\n",
    "prediction_cols.append(\"equal_weight\")\n",
    "\n",
    "#50% neutralized ensemble - So we can reduce dependency on certain features - Reducing overfitting\n",
    "all_data[\"half_neutral_equal_weight\"] = neutralize(\n",
    "    df=all_data.loc[validation_index, :],\n",
    "    columns=[f\"equal_weight\"],\n",
    "    neutralizers=features,\n",
    "    proportion=0.5,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL,\n",
    "    verbose=True,\n",
    ")\n",
    "# do the same for live data\n",
    "live_data[\"half_neutral_equal_weight\"] = neutralize(\n",
    "    df=live_data,\n",
    "    columns=[f\"equal_weight\"],\n",
    "    neutralizers=features,\n",
    "    proportion=0.5,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols.append(\"half_neutral_equal_weight\")\n",
    "\n",
    "model_to_submit = f\"half_neutral_equal_weight\"\n",
    "# rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n",
    "all_data.loc[validation_index, \"prediction\"] = all_data.loc[\n",
    "    validation_index, model_to_submit\n",
    "].rank(pct=True)\n",
    "live_data[\"prediction\"] = live_data[model_to_submit].rank(pct=True)\n",
    "all_data.loc[validation_index, \"prediction\"].to_csv(\n",
    "    f\"validation_predictions_{current_round}.csv\"\n",
    ")\n",
    "live_data[\"prediction\"].to_csv(f\"live_predictions_{current_round}.csv\")\n",
    "\n",
    "validation_example_preds = pd.read_parquet(\n",
    "    f\"{version}/validation_example_preds.parquet\"\n",
    ")\n",
    "all_data.loc[validation_index, EXAMPLE_PREDS_COL] = validation_example_preds[\n",
    "    \"prediction\"\n",
    "]\n",
    "# fast_mode=True so that we skip some of the stats that are slower to calculate\n",
    "validation_stats = validation_metrics(\n",
    "    all_data.loc[validation_index, :],\n",
    "    prediction_cols,\n",
    "    example_col=EXAMPLE_PREDS_COL,\n",
    "    fast_mode=True,\n",
    "    target_col=TARGET_COL,\n",
    ")\n",
    "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "Done! Next steps:\n",
    "    1. Go to numer.ai/tournament (make sure you have an account)\n",
    "    2. Submit validation_predictions_{current_round}.csv to the diagnostics tool\n",
    "    3. Submit tournament_predictions_{current_round}.csv to the \"Upload Predictions\" button\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
