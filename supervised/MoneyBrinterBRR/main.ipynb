{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from numerapi import NumerAPI\n",
    "from utils import (\n",
    "    save_model,\n",
    "    load_model,\n",
    "    neutralize,\n",
    "    validation_metrics,\n",
    "    ERA_COL,\n",
    "    DATA_TYPE_COL,\n",
    "    TARGET_COL,\n",
    "    EXAMPLE_PREDS_COL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "\r"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 10:24:14,502 INFO numerapi.utils: target file already exists\n",
      "2023-07-10 10:24:14,503 INFO numerapi.utils: download complete\n",
      "2023-07-10 10:24:15,681 INFO numerapi.utils: target file already exists\n",
      "2023-07-10 10:24:15,682 INFO numerapi.utils: download complete\n",
      "2023-07-10 10:24:16,923 INFO numerapi.utils: target file already exists\n",
      "2023-07-10 10:24:16,924 INFO numerapi.utils: download complete\n",
      "2023-07-10 10:24:18,146 INFO numerapi.utils: target file already exists\n",
      "2023-07-10 10:24:18,147 INFO numerapi.utils: download complete\n",
      "2023-07-10 10:24:19,226 INFO numerapi.utils: target file already exists\n",
      "2023-07-10 10:24:19,227 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "napi = NumerAPI()\n",
    "current_round = napi.get_current_round()\n",
    "#Dataset path\n",
    "version = \"v4.1\"\n",
    "feature_set_name = \"medium\"\n",
    "train_path = f\"{version}/train.parquet\"\n",
    "validation_path = f\"{version}/validation.parquet\"\n",
    "\n",
    "# download all the things\n",
    "napi.download_dataset(f\"{version}/train_int8.parquet\")\n",
    "napi.download_dataset(f\"{version}/validation_int8.parquet\")\n",
    "napi.download_dataset(\n",
    "    f\"{version}/live_int8.parquet\",\n",
    "    f\"{version}/live_int8_{current_round}.parquet\",\n",
    ")\n",
    "\n",
    "napi.download_dataset(f\"{version}/validation_example_preds.parquet\")\n",
    "napi.download_dataset(f\"{version}/features.json\")\n",
    "\n",
    "with open(f\"{version}/features.json\", \"r\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "\n",
    "# features = list(feature_metadata[\"feature_stats\"].keys()) # get all the features\n",
    "# features = feature_metadata[\"feature_sets\"][\"small\"] # get the small feature set\n",
    "features = feature_metadata[\"feature_sets\"][\n",
    "    feature_set_name\n",
    "]  # get the medium feature set\n",
    "target_cols = feature_metadata[\"targets\"]\n",
    "# read in just those features along with era and target columns\n",
    "read_columns = features + target_cols + [ERA_COL, DATA_TYPE_COL]\n",
    "\n",
    "# note: sometimes when trying to read the downloaded data you get an error about invalid magic parquet bytes...\n",
    "# if so, delete the file and rerun the napi.download_dataset to fix the corrupted file\n",
    "training_data = pd.read_parquet(\n",
    "    f\"{version}/train_int8.parquet\", columns=read_columns\n",
    ")\n",
    "validation_data = pd.read_parquet(\n",
    "    f\"{version}/validation_int8.parquet\", columns=read_columns\n",
    ")\n",
    "live_data = pd.read_parquet(f\"{version}/live_int8_{current_round}.parquet\", columns=read_columns)\n",
    "\n",
    "\n",
    "# get all the data to possibly use for training\n",
    "all_data = pd.concat([training_data, validation_data])\n",
    "\n",
    "# save indices for easier data selection later\n",
    "training_index = training_data.index\n",
    "validation_index = validation_data.index\n",
    "all_index = all_data.index\n",
    "\n",
    "# delete training and validation data to save space\n",
    "del training_data\n",
    "del validation_data\n",
    "gc.collect()  # clear up memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning up NAs\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Int8 datatype has pd.NA which don't play nice with models.  We simply fill NA with median values here\n",
    "print(\"cleaning up NAs\")\n",
    "all_data[features] = all_data[features].fillna(all_data[features].median(skipna=True))\n",
    "all_data[features] = all_data[features].astype(\"int8\")  # make sure change to float32 if using the non int8 data!\n",
    "live_data[features] = live_data[features].fillna(\n",
    "    all_data[features].median(skipna=True)\n",
    ")  # since live data is only one era, we need to use the median for all eras\n",
    "live_data[features] = live_data[features].astype(\"int8\")  # make sure change to float32 if using the non int8 data!\n",
    "# Alternatively could convert nan columns to be floats and replace pd.NA with np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "targets = [\n",
    "    \"target_nomi_v4_20\",\n",
    "    \"target_jerome_v4_60\",\n",
    "    \"target_ralph_v4_20\",\n",
    "    \"target_tyler_v4_20\",\n",
    "    \"target_victor_v4_20\",\n",
    "    \"target_waldo_v4_20\",\n",
    "]\n",
    "params_name = \"lg_lgbm\"\n",
    "params = {\n",
    "    \"n_estimators\": 20000,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_depth\": 6,\n",
    "    \"num_leaves\": 2**6,\n",
    "    \"colsample_bytree\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cols = []\n",
    "for target in tqdm(targets):\n",
    "    prediction_col = f\"{params_name}_{version}_{feature_set_name}_{target}\"\n",
    "    train_data_model_name = f\"train_data_{prediction_col}\"\n",
    "    print(f\"Checking for existing model '{train_data_model_name}'\")\n",
    "    train_model = load_model(train_data_model_name)\n",
    "    if not train_model:\n",
    "        print(f\"model not found, creating new one\")\n",
    "        train_model = LGBMRegressor(**params)\n",
    "        # train on all of train and save the model so we don't have to train next time\n",
    "        target_train_index = (\n",
    "            all_data.loc[training_index, target].dropna().index\n",
    "        )  # make sure we only train on rows which have this target\n",
    "        train_model.fit(\n",
    "            all_data.loc[target_train_index, features],\n",
    "            all_data.loc[target_train_index, target],\n",
    "        )  # in case some of the targets are missing data\n",
    "        print(f\"saving new model: {train_data_model_name}\")\n",
    "        save_model(train_model, train_data_model_name)\n",
    "\n",
    "    # predict on validation data\n",
    "    all_data.loc[validation_index, prediction_col] = train_model.predict(\n",
    "        all_data.loc[validation_index, features]\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "    # do the same thing for all data (for predicting on live)\n",
    "    all_data_model_name = f\"all_data_{prediction_col}\"\n",
    "    print(f\"Checking for existing model '{all_data_model_name}'\")\n",
    "    all_data_model = load_model(all_data_model_name)\n",
    "    if not all_data_model:\n",
    "        print(f\"model not found, creating new one\")\n",
    "        all_data_model = LGBMRegressor(**params)\n",
    "        all_data_target_index = (\n",
    "            all_data.loc[all_index, target].dropna().index\n",
    "        )  # make sure we only train on rows which have this target\n",
    "        # train on all of train and save the model so we don't have to train next time\n",
    "        all_data_model.fit(\n",
    "            all_data.loc[all_data_target_index, features],\n",
    "            all_data.loc[all_data_target_index, target],\n",
    "        )\n",
    "        print(f\"saving new model: {all_data_model_name}\")\n",
    "        save_model(all_data_model, all_data_model_name)\n",
    "\n",
    "    # predict on live data\n",
    "    live_data[prediction_col] = all_data_model.predict(\n",
    "        live_data[features].fillna(np.nan)\n",
    "    )  # filling live data with nans makes us ignore those features if necessary\n",
    "    gc.collect()\n",
    "\n",
    "    prediction_cols.append(prediction_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_only_highest(current_accuracy, epoch):\n",
    "    model_path = f\"../models/MoneyBrinterBRR/acc:{current_accuracy:.2f}| epoch:{epoch}\"\n",
    "    filenames = os.listdir(model_path)\n",
    "    highest_accuracy = max([float(str.split(filename, ':')[1]) for filename in filenames])\n",
    "\n",
    "    if current_accuracy > highest_accuracy:\n",
    "        filepath_with_highest_accuracy = f\"../models/dogs-cats/dogs-cats-cnn.pth-accurracy-{highest_accuracy}\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        os.remove(filepath_with_highest_accuracy) \n",
    "        print(f\"Saved the highest accuracy model to {model_path}, with accuracy {current_accuracy:.2f}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
